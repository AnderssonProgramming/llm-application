package edu.study.llm_application.infrastructure.adapters.out.mock;

import edu.study.llm_application.domain.entities.LlmRequest;
import edu.study.llm_application.domain.entities.LlmResponse;
import edu.study.llm_application.domain.ports.out.LlmProviderPort;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.context.annotation.Primary;
import org.springframework.stereotype.Component;

import java.util.Random;

/**
 * Mock LLM provider for testing and demo purposes
 * Activated when openai.mock.enabled=true
 */
@Slf4j
@Component
@Primary
@ConditionalOnProperty(name = "openai.mock.enabled", havingValue = "true", matchIfMissing = true)
public class MockLlmAdapter implements LlmProviderPort {
    
    private final Random random = new Random();
    
    private final String[] mockResponses = {
        "This is a mock response from the LLM. In a real implementation, this would be generated by an actual language model like GPT.",
        "Hello! I'm a simulated AI assistant. This response is generated by a mock adapter for demonstration purposes.",
        "Thank you for your question! This is a sample response that demonstrates how the LLM application works without requiring an actual API key.",
        "I understand your request. This mock response shows the structure and flow of the application while using a simulated LLM provider.",
        "Great question! This response is coming from a mock LLM adapter, which is useful for development and testing purposes."
    };
    
    @Override
    public LlmResponse generateResponse(LlmRequest request) throws LlmProviderException {
        log.info("Generating mock response for prompt: {}", 
                request.getPrompt().substring(0, Math.min(50, request.getPrompt().length())));
        
        try {
            // Simulate processing time
            Thread.sleep(500 + random.nextInt(1500)); // 0.5-2 seconds
            
            // Generate mock response
            String mockContent = generateMockContent(request.getPrompt());
            int mockTokens = 20 + random.nextInt(100); // 20-120 tokens
            
            return LlmResponse.success(
                    request.getId(),
                    mockContent,
                    request.getModel(),
                    mockTokens,
                    null // Will be set by use case
            );
            
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new LlmProviderException("Mock processing interrupted", e);
        }
    }
    
    @Override
    public boolean isHealthy() {
        return true; // Mock is always healthy
    }
    
    @Override
    public String[] getSupportedModels() {
        return new String[]{
            "gpt-3.5-turbo",
            "gpt-4",
            "mock-model-v1",
            "demo-llm"
        };
    }
    
    @Override
    public String getProviderName() {
        return "Mock LLM Provider";
    }
    
    private String generateMockContent(String prompt) {
        // Select a base response
        String baseResponse = mockResponses[random.nextInt(mockResponses.length)];
        
        // Add some context based on the prompt
        String contextualPrefix = "";
        String lowerPrompt = prompt.toLowerCase();
        
        if (lowerPrompt.contains("hello") || lowerPrompt.contains("hi")) {
            contextualPrefix = "Hello there! ";
        } else if (lowerPrompt.contains("explain") || lowerPrompt.contains("what")) {
            contextualPrefix = "Let me explain: ";
        } else if (lowerPrompt.contains("help")) {
            contextualPrefix = "I'd be happy to help! ";
        } else if (lowerPrompt.contains("code") || lowerPrompt.contains("program")) {
            contextualPrefix = "Regarding programming: ";
        }
        
        return contextualPrefix + baseResponse + 
               "\n\n(Note: This is a mock response. The prompt was: \"" + 
               (prompt.length() > 100 ? prompt.substring(0, 100) + "..." : prompt) + "\")";
    }
}
